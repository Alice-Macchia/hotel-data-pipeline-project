{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b703fa50-58b2-4569-a15d-268d167f70f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "      <div style=\"font-size:18px\">\n",
       "        The Delta Live Tables (DLT) module is not supported on this cluster.\n",
       "        You should either <a href=\"?o=3032302114641454#joblist/pipelines/create?initialSource=%2FUsers%2Falimacchia%40gmail.com%2Fprogetto_finale%2Fbronze_ingestion&redirectNotebookId=1821690743053800\">create a new pipeline</a> or use an existing pipeline to run DLT code.\n",
       "      </div>\n",
       "    </html>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-8868909947312942>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdlt\u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Percorso del volume contenente i file CSV sorgente\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'dlt'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ModuleNotFoundError",
        "evalue": "No module named 'dlt'"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>ModuleNotFoundError</span>: No module named 'dlt'"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
        "File \u001B[0;32m<command-8868909947312942>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdlt\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Percorso del volume contenente i file CSV sorgente\u001B[39;00m\n",
        "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'dlt'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Percorso del volume dove hai caricato i file CSV\n",
    "source_files_path = \"/Volumes/catalog_progetto_finale/schema_pf/volume_pf\"\n",
    "\n",
    "# @dlt -> decoratore che intercetta la funzione al suo interno e la trasforma in un task all'interno di una pipeline\n",
    "@dlt.table(name=\"hotels_bronze\", comment=\"Dati grezzi degli hotel.\")\n",
    "def hotels_bronze():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\") #pipeline incrementale: questo formato attiva AutoLoader che controlla se ci sono nuovi file o file modificati\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"pathGlobFilter\", \"hotels.csv\") \n",
    "        .load(source_files_path)\n",
    "        .withColumn(\"ingestion_date\", current_timestamp()) # aggiunge una nuova colonna che contiene la data e l'ora esatta in cui i dati sono stati processati\n",
    "    )\n",
    "\n",
    "@dlt.table(name=\"customers_bronze\", comment=\"Dati grezzi dei clienti.\")\n",
    "def customers_bronze():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"pathGlobFilter\", \"customers.csv\")\n",
    "        .load(source_files_path)\n",
    "        .withColumn(\"ingestion_date\", current_timestamp())\n",
    "    )\n",
    "\n",
    "@dlt.table(name=\"rooms_bronze\", comment=\"Dati grezzi delle stanze.\")\n",
    "def rooms_bronze():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"pathGlobFilter\", \"rooms.csv\")\n",
    "        .load(source_files_path)\n",
    "        .withColumn(\"ingestion_date\", current_timestamp())\n",
    "    )\n",
    "\n",
    "@dlt.table(name=\"bookings_bronze\", comment=\"Dati grezzi delle prenotazioni.\")\n",
    "def bookings_bronze():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"pathGlobFilter\", \"bookings.csv\")\n",
    "        .load(source_files_path)\n",
    "        .withColumn(\"ingestion_date\", current_timestamp())\n",
    "    )\n",
    "\n",
    "@dlt.table(name=\"payments_bronze\", comment=\"Dati grezzi dei pagamenti.\")\n",
    "def payments_bronze():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"pathGlobFilter\", \"payments.csv\")\n",
    "        .load(source_files_path)\n",
    "        .withColumn(\"ingestion_date\", current_timestamp())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5eee7a2-dc30-4c0e-b303-f2d75f3c719e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Il codice definisce una pipeline che:\n",
    "\n",
    "Per ogni tabella, usa Auto Loader per monitorare in modo efficiente una cartella.\n",
    "\n",
    "Grazie al filtro, seleziona solo il file CSV corretto per quella tabella.\n",
    "\n",
    "Legge tutti i dati come stringhe per massimizzare l'affidabilit√†.\n",
    "\n",
    "Aggiunge un timestamp di ingestione per sapere quando i dati sono stati caricati.\n",
    "\n",
    "DLT orchestra il tutto, creando o aggiornando la tabella Delta di destinazione (es. hotels_bronze) con i nuovi dati trovati."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}